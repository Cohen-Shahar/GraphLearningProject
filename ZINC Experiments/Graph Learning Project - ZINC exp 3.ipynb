{"cells":[{"cell_type":"markdown","metadata":{"id":"XuXWJLEm2UWS"},"source":["# **Graph Learning Project - ZINC exp 3**"]},{"cell_type":"markdown","metadata":{"id":"8gzsP50bF6Gb"},"source":["By Shahar Cohen 205669260 & Alexander petrunin 205782568"]},{"cell_type":"markdown","metadata":{"id":"67gOQITlCNQi"},"source":["# Installation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"J_m9l6OYCQZP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731828057973,"user_tz":-120,"elapsed":3983,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"c35bc1cd-2efc-4992-81a1-5ec5359520a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch-geometric"]},{"cell_type":"markdown","metadata":{"id":"9V_JqF8wNr1q"},"source":["# SETUP"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PRfgbfTjCRD_","executionInfo":{"status":"ok","timestamp":1731828069814,"user_tz":-120,"elapsed":11843,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch_geometric.nn import GPSConv, GatedGraphConv, TransformerConv, GINEConv\n","from torch_geometric.data import Data\n","import torch.nn.functional as F\n","from torch_geometric.nn import global_mean_pool, global_add_pool\n","\n","from torch_geometric.transforms import AddLaplacianEigenvectorPE\n","import torch_geometric\n","\n","import torch.optim as optim\n","\n","from torch_geometric.datasets import ZINC\n","from torch_geometric.loader import DataLoader\n","\n","import torch.optim as optim\n","from torch_geometric.data import DataLoader\n","from sklearn.metrics import mean_squared_error\n","\n","from torch_geometric.transforms import AddRandomWalkPE\n","from torch_geometric.datasets import ZINC\n","from torch_geometric.data import DataLoader\n","\n","from torch_geometric.typing import Tensor\n","from torch_geometric.typing import Adj\n","from typing import Any, Dict, Optional\n","from torch_geometric.utils import to_dense_batch\n","from torch_geometric.nn.attention import PerformerAttention\n"]},{"cell_type":"markdown","metadata":{"id":"nxbRfIBwj8v2"},"source":["# MODEL:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WLjIZKJZqrk5","executionInfo":{"status":"ok","timestamp":1731828069814,"user_tz":-120,"elapsed":3,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["class MLPBlock(nn.Module):\n","    def __init__(self, in_channels, hidden_channels):\n","        super(MLPBlock, self).__init__()\n","        self.fc1 = nn.Linear(in_channels, hidden_channels)\n","        self.fc2 = nn.Linear(hidden_channels, hidden_channels)  # This should output 'hidden_channels'\n","\n","    def forward(self, x):\n","        x = x.float()  # Ensure the input is float before passing it to the linear layer\n","        x = F.relu(self.fc1(x))  # Apply ReLU activation after the first linear layer\n","        x = self.fc2(x)  # The second layer keeps the number of features as hidden_channels\n","        return x\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uf4G1gD1SLTZ","executionInfo":{"status":"ok","timestamp":1731828069814,"user_tz":-120,"elapsed":3,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["class GraphGPSModel(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim, pe_in_dim, pe_out_dim, num_layers):\n","        super(GraphGPSModel, self).__init__()\n","\n","        # MLP layers\n","        self.mlp1 = MLPBlock(input_dim + pe_out_dim, hidden_dim)\n","\n","        # Create MLP layers for GINEConv GPSConv layers\n","        self.mlps = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(hidden_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, hidden_dim)\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Create GPSConv layers\n","        self.gps_layers = nn.ModuleList([\n","            CustomGPSConv(\n","                hidden_dim,\n","                conv=GINEConv(self.mlps[i], eps=0.0, train_eps=False, edge_dim=3),\n","                heads=4,\n","                attn_kwargs={'dropout': 0.5}\n","            )\n","            for i in range(num_layers)\n","        ])\n","\n","        # Final fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","        # PE layers\n","        self.bn_pe = nn.BatchNorm1d(pe_in_dim)\n","        self.fc_pe = nn.Linear(pe_in_dim, pe_out_dim)\n","\n","\n","\n","    def forward(self, data):\n","\n","        x, edge_index, batch, pe, edge_attr = (\n","            data.x,\n","            data.edge_index,\n","            data.batch,\n","            data.random_walk_pe,\n","            data.edge_attr\n","        )\n","\n","        # Transform edge attributes\n","        dummy_tensor = torch.zeros(edge_attr.size(0), 3, device=edge_attr.device)  # Change size to 3 for the new dummy\n","        dummy_tensor[edge_attr == 1, 0] = 1  # Keep the condition for edge_attr == 1\n","        dummy_tensor[edge_attr == 2, 1] = 1  # Keep the condition for edge_attr == 2\n","        dummy_tensor[edge_attr == 3, 2] = 1  # New condition for edge_attr == 3\n","        edge_attr = dummy_tensor\n","\n","\n","        # Process positional encodings (PE)\n","        pe = self.bn_pe(pe)\n","        pe = self.fc_pe(pe)\n","\n","        # Concatenate PE to node features\n","        x = torch.cat([x, pe], dim=1)\n","\n","        # Initial MLP processing\n","        x = self.mlp1(x)\n","\n","        # Sequentially apply GPSConv layers\n","        for gps_layer in self.gps_layers:\n","            x = gps_layer(x, edge_index, batch=batch, edge_attr=edge_attr)\n","\n","        # Global pooling to aggregate node features into graph features\n","        x = global_add_pool(x, batch)\n","\n","        # Final classification layer\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"-22Lvmw4STN0"},"source":["GPS layer"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7jjLxaasSSHb","executionInfo":{"status":"ok","timestamp":1731828069814,"user_tz":-120,"elapsed":3,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["class CustomGPSConv(GPSConv):\n","    def __init__(self,hidden_dim, *args, **kwargs):\n","        super().__init__(hidden_dim, *args, **kwargs)\n","        self.MLP_combine = MLPBlock(hidden_dim*2, hidden_dim)\n","        self.cross_attn1 = torch.nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=2)\n","        self.cross_attn2 = torch.nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=2)\n","\n","    def forward(\n","        self,\n","        x: Tensor,\n","        edge_index: Adj,\n","        batch: Optional[torch.Tensor] = None,\n","        **kwargs,\n","    ) -> Tensor:\n","        r\"\"\"Runs the forward pass of the module.\"\"\"\n","        hs = []\n","        if self.conv is not None:  # Local MPNN.\n","            h = self.conv(x, edge_index, **kwargs)\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","            h = h + x\n","            if self.norm1 is not None:\n","                if self.norm_with_batch:\n","                    h = self.norm1(h, batch=batch)\n","                else:\n","                    h = self.norm1(h)\n","            hs.append(h)\n","\n","        # Global attention transformer-style model.\n","        h, mask = to_dense_batch(x, batch)\n","\n","        if isinstance(self.attn, torch.nn.MultiheadAttention):\n","            h, _ = self.attn(h, h, h, key_padding_mask=~mask,\n","                             need_weights=False)\n","        elif isinstance(self.attn, PerformerAttention):\n","            h = self.attn(h, mask=mask)\n","\n","        h = h[mask]\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","        h = h + x  # Residual connection.\n","        if self.norm2 is not None:\n","            if self.norm_with_batch:\n","                h = self.norm2(h, batch=batch)\n","            else:\n","                h = self.norm2(h)\n","        hs.append(h)\n","\n","\n","        ##################\n","\n","        # Original Code:\n","        #out = sum(hs)  # Combine local and global outputs.\n","\n","        # Our code:\n","\n","        # Combine the two tensors in hs with cross attention layer\n","        cross_attn_output1, _ = self.cross_attn1(\n","            query=hs[0].unsqueeze(1),\n","            key=hs[1].unsqueeze(1),\n","            value=hs[1].unsqueeze(1)\n","        )\n","\n","        cross_attn_output2, _ = self.cross_attn2(\n","            query=hs[1].unsqueeze(1),\n","            key=hs[0].unsqueeze(1),\n","            value=hs[0].unsqueeze(1)\n","        )\n","\n","        cross_attn_output1 = cross_attn_output1.squeeze(1)  # Back to (N, hidden_dim)\n","        cross_attn_output1 = 0.1 * cross_attn_output1 + hs[1]\n","        cross_attn_output2 = cross_attn_output2.squeeze(1)\n","        cross_attn_output1 = 0.1 * cross_attn_output2 + hs[0]\n","\n","        out = torch.cat([cross_attn_output1, cross_attn_output2], dim=1)\n","\n","        # Pass the combined tensor through the MLP\n","        out = self.MLP_combine(out) + sum(hs)\n","        #+ sum(hs)\n","\n","        # Original Code:\n","        #out = out + self.mlp(out)\n","\n","        #################\n","\n","\n","        if self.norm3 is not None:\n","            if self.norm_with_batch:\n","                out = self.norm3(out, batch=batch)\n","            else:\n","                out = self.norm3(out)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"tmg5-2VbSSvX"},"source":["# Load ZINC and add PE:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ONAcZhFhkXwC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731828158814,"user_tz":-120,"elapsed":89003,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"d07070c3-383b-4d98-dbd1-9f086f41db63"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.dropbox.com/s/feo9qle74kg48gy/molecules.zip?dl=1\n","Extracting data/molecules.zip\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/train.index\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/val.index\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/test.index\n","Processing...\n","Processing train dataset: 100%|██████████| 10000/10000 [00:00<00:00, 12232.95it/s]\n","Processing val dataset: 100%|██████████| 1000/1000 [00:00<00:00, 3901.33it/s]\n","Processing test dataset: 100%|██████████| 1000/1000 [00:00<00:00, 8784.43it/s]\n","Done!\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}],"source":["# Load the ZINC dataset\n","transform = AddRandomWalkPE(walk_length=20)\n","\n","# Load the ZINC dataset with predefined splits\n","train_dataset = ZINC(root='./data', subset=True, split='train', transform=transform)\n","val_dataset = ZINC(root='./data', subset=True, split='val', transform=transform)\n","test_dataset = ZINC(root='./data', subset=True, split='test', transform=transform)\n","\n","# Create DataLoaders for batching\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"5fhLOTceSr9p"},"source":["# Training"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WLHHpsekXog2","executionInfo":{"status":"ok","timestamp":1731828158814,"user_tz":-120,"elapsed":2,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["# Training loop\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        optimizer.zero_grad()\n","\n","        # Move data to the same device as the model\n","        data = data.to(device)\n","\n","        # Forward pass\n","        output = model(data)\n","\n","        # Get the target values (penalized logP)\n","        y = data.y.view(-1, 1).to(device)  # Ensure target is on the same device as the model\n","\n","        # Compute the loss\n","        loss = criterion(output, y)\n","        loss.backward()\n","\n","        # Optimization step\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","\n","    return total_loss / len(train_loader)\n","\n","\n","# Define a function to evaluate the model on a given dataset\n","def evaluate(loader):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():  # Disable gradient computation for evaluation\n","        for data in loader:\n","            data = data.to(device)\n","            output = model(data)\n","            y = data.y.view(-1, 1).to(device)\n","            loss = criterion(output, y)\n","            total_loss += loss.item()\n","    return total_loss / len(loader)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"vaMtoPSbVADf","executionInfo":{"status":"ok","timestamp":1731828158814,"user_tz":-120,"elapsed":2,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["num_layers = 10\n","input_dim = train_dataset.num_features\n","hidden_dim = 64\n","output_dim = 1\n","pe_in_dim = 20\n","pe_out_dim = 28\n","\n","weight_decay = 1e-5\n","lr = 0.001\n","epochs_num = 250"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__Xx9j2RVAFy","executionInfo":{"status":"ok","timestamp":1731836123720,"user_tz":-120,"elapsed":7964907,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"23af0e2e-c52b-428b-af02-91d39fa72c1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 3.6225, Validation Loss: 1.7673\n","New best model saved at epoch 1 with Validation Loss: 1.7673\n","Epoch 2, Train Loss: 1.4057, Validation Loss: 1.6800\n","New best model saved at epoch 2 with Validation Loss: 1.6800\n","Epoch 3, Train Loss: 1.2610, Validation Loss: 1.1061\n","New best model saved at epoch 3 with Validation Loss: 1.1061\n","Epoch 4, Train Loss: 1.1169, Validation Loss: 1.0309\n","New best model saved at epoch 4 with Validation Loss: 1.0309\n","Epoch 5, Train Loss: 0.9928, Validation Loss: 1.1098\n","Epoch 6, Train Loss: 1.0057, Validation Loss: 0.9016\n","New best model saved at epoch 6 with Validation Loss: 0.9016\n","Epoch 7, Train Loss: 0.9660, Validation Loss: 1.2187\n","Epoch 8, Train Loss: 0.8776, Validation Loss: 1.1113\n","Epoch 9, Train Loss: 0.9408, Validation Loss: 0.9151\n","Epoch 10, Train Loss: 0.8528, Validation Loss: 0.9013\n","New best model saved at epoch 10 with Validation Loss: 0.9013\n","Epoch 11, Train Loss: 0.8113, Validation Loss: 1.0602\n","Epoch 12, Train Loss: 0.8714, Validation Loss: 0.9036\n","Epoch 13, Train Loss: 0.8601, Validation Loss: 1.1734\n","Epoch 14, Train Loss: 0.8139, Validation Loss: 1.0045\n","Epoch 15, Train Loss: 0.7655, Validation Loss: 0.8535\n","New best model saved at epoch 15 with Validation Loss: 0.8535\n","Epoch 16, Train Loss: 0.7698, Validation Loss: 0.8403\n","New best model saved at epoch 16 with Validation Loss: 0.8403\n","Epoch 17, Train Loss: 0.7423, Validation Loss: 1.4319\n","Epoch 18, Train Loss: 0.8690, Validation Loss: 0.8416\n","Epoch 19, Train Loss: 0.8349, Validation Loss: 0.8077\n","New best model saved at epoch 19 with Validation Loss: 0.8077\n","Epoch 20, Train Loss: 0.7851, Validation Loss: 0.9736\n","Epoch 21, Train Loss: 0.8236, Validation Loss: 0.8686\n","Epoch 22, Train Loss: 0.7093, Validation Loss: 0.8570\n","Epoch 23, Train Loss: 0.7676, Validation Loss: 1.2835\n","Epoch 24, Train Loss: 0.8051, Validation Loss: 0.9365\n","Epoch 25, Train Loss: 0.8077, Validation Loss: 1.0850\n","Epoch 26, Train Loss: 0.7143, Validation Loss: 1.7344\n","Epoch 27, Train Loss: 0.7636, Validation Loss: 0.8420\n","Epoch 28, Train Loss: 0.7255, Validation Loss: 0.8238\n","Epoch 29, Train Loss: 0.6975, Validation Loss: 0.7620\n","New best model saved at epoch 29 with Validation Loss: 0.7620\n","Epoch 30, Train Loss: 0.8309, Validation Loss: 0.9245\n","Epoch 31, Train Loss: 0.7217, Validation Loss: 0.8885\n","Epoch 32, Train Loss: 0.6728, Validation Loss: 1.2215\n","Epoch 33, Train Loss: 0.8337, Validation Loss: 1.0441\n","Epoch 34, Train Loss: 0.7981, Validation Loss: 0.8200\n","Epoch 35, Train Loss: 0.7144, Validation Loss: 0.8167\n","Epoch 36, Train Loss: 0.7809, Validation Loss: 0.7107\n","New best model saved at epoch 36 with Validation Loss: 0.7107\n","Epoch 37, Train Loss: 0.6337, Validation Loss: 1.3066\n","Epoch 38, Train Loss: 0.7963, Validation Loss: 0.8833\n","Epoch 39, Train Loss: 0.7367, Validation Loss: 0.7758\n","Epoch 40, Train Loss: 0.7218, Validation Loss: 0.7833\n","Epoch 41, Train Loss: 0.7218, Validation Loss: 0.7803\n","Epoch 42, Train Loss: 0.7816, Validation Loss: 1.2366\n","Epoch 43, Train Loss: 0.8231, Validation Loss: 0.7548\n","Epoch 44, Train Loss: 0.9383, Validation Loss: 1.9380\n","Epoch 45, Train Loss: 1.0749, Validation Loss: 2.5039\n","Epoch 46, Train Loss: 2.3827, Validation Loss: 2.0623\n","Epoch 47, Train Loss: 1.8043, Validation Loss: 1.9343\n","Epoch 48, Train Loss: 1.6177, Validation Loss: 1.4809\n","Epoch 49, Train Loss: 1.4668, Validation Loss: 1.4526\n","Epoch 50, Train Loss: 1.3423, Validation Loss: 1.2231\n","Epoch 51, Train Loss: 1.2327, Validation Loss: 1.3297\n","Epoch 52, Train Loss: 1.2361, Validation Loss: 1.3334\n","Epoch 53, Train Loss: 1.0691, Validation Loss: 0.9802\n","Epoch 54, Train Loss: 1.0065, Validation Loss: 1.0220\n","Epoch 55, Train Loss: 0.9528, Validation Loss: 0.9749\n","Epoch 56, Train Loss: 0.9754, Validation Loss: 1.1754\n","Epoch 57, Train Loss: 0.9210, Validation Loss: 0.9900\n","Epoch 58, Train Loss: 0.9051, Validation Loss: 1.3963\n","Epoch 59, Train Loss: 0.8826, Validation Loss: 1.8302\n","Epoch 60, Train Loss: 0.8892, Validation Loss: 1.0055\n","Epoch 61, Train Loss: 0.8932, Validation Loss: 0.8025\n","Epoch 62, Train Loss: 0.8474, Validation Loss: 0.9915\n","Epoch 63, Train Loss: 0.8252, Validation Loss: 0.8346\n","Epoch 64, Train Loss: 0.8599, Validation Loss: 1.2976\n","Epoch 65, Train Loss: 0.8105, Validation Loss: 0.8084\n","Epoch 66, Train Loss: 0.8013, Validation Loss: 0.9299\n","Epoch 67, Train Loss: 0.8187, Validation Loss: 0.9628\n","Epoch 68, Train Loss: 0.8486, Validation Loss: 0.7656\n","Epoch 69, Train Loss: 0.7633, Validation Loss: 0.8173\n","Epoch 70, Train Loss: 0.8415, Validation Loss: 1.0412\n","Epoch 71, Train Loss: 0.8237, Validation Loss: 0.8216\n","Epoch 72, Train Loss: 0.8086, Validation Loss: 1.0034\n","Epoch 73, Train Loss: 0.8399, Validation Loss: 1.0132\n","Epoch 74, Train Loss: 0.8256, Validation Loss: 0.7497\n","Epoch 75, Train Loss: 0.7509, Validation Loss: 0.8707\n","Epoch 76, Train Loss: 0.7738, Validation Loss: 0.6983\n","New best model saved at epoch 76 with Validation Loss: 0.6983\n","Epoch 77, Train Loss: 0.7165, Validation Loss: 0.7186\n","Epoch 78, Train Loss: 0.7709, Validation Loss: 1.1271\n","Epoch 79, Train Loss: 0.8790, Validation Loss: 0.8184\n","Epoch 80, Train Loss: 0.7580, Validation Loss: 0.9282\n","Epoch 81, Train Loss: 0.8999, Validation Loss: 0.8559\n","Epoch 82, Train Loss: 0.7343, Validation Loss: 0.9210\n","Epoch 83, Train Loss: 2.1775, Validation Loss: 4.2947\n","Epoch 84, Train Loss: 2.4330, Validation Loss: 2.9606\n","Epoch 85, Train Loss: 2.3633, Validation Loss: 2.4358\n","Epoch 86, Train Loss: 2.3554, Validation Loss: 2.5013\n","Epoch 87, Train Loss: 2.3293, Validation Loss: 2.5170\n","Epoch 88, Train Loss: 2.3199, Validation Loss: 2.2729\n","Epoch 89, Train Loss: 2.3072, Validation Loss: 2.6297\n","Epoch 90, Train Loss: 2.2920, Validation Loss: 4.7922\n","Epoch 91, Train Loss: 2.2758, Validation Loss: 4.7988\n","Epoch 92, Train Loss: 2.3012, Validation Loss: 2.5647\n","Epoch 93, Train Loss: 2.3037, Validation Loss: 2.2122\n","Epoch 94, Train Loss: 2.2803, Validation Loss: 2.2052\n","Epoch 95, Train Loss: 2.2582, Validation Loss: 2.2791\n","Epoch 96, Train Loss: 2.2418, Validation Loss: 2.2012\n","Epoch 97, Train Loss: 2.2585, Validation Loss: 2.3115\n","Epoch 98, Train Loss: 2.2145, Validation Loss: 2.8691\n","Epoch 99, Train Loss: 2.2533, Validation Loss: 4.4758\n","Epoch 100, Train Loss: 2.2137, Validation Loss: 3.2878\n","Epoch 101, Train Loss: 2.2071, Validation Loss: 2.1502\n","Epoch 102, Train Loss: 2.2241, Validation Loss: 2.2003\n","Epoch 103, Train Loss: 2.1934, Validation Loss: 2.2096\n","Epoch 104, Train Loss: 2.2006, Validation Loss: 3.2812\n","Epoch 105, Train Loss: 2.1830, Validation Loss: 2.1202\n","Epoch 106, Train Loss: 2.1800, Validation Loss: 2.1317\n","Epoch 107, Train Loss: 2.1496, Validation Loss: 2.2138\n","Epoch 108, Train Loss: 2.1246, Validation Loss: 2.2522\n","Epoch 109, Train Loss: 2.1749, Validation Loss: 2.2466\n","Epoch 110, Train Loss: 2.1314, Validation Loss: 2.0116\n","Epoch 111, Train Loss: 2.1422, Validation Loss: 2.0779\n","Epoch 112, Train Loss: 2.1373, Validation Loss: 2.2412\n","Epoch 113, Train Loss: 2.1582, Validation Loss: 2.1835\n","Epoch 114, Train Loss: 2.1131, Validation Loss: 2.0128\n","Epoch 115, Train Loss: 2.1211, Validation Loss: 2.0433\n","Epoch 116, Train Loss: 2.0569, Validation Loss: 2.2107\n","Epoch 117, Train Loss: 2.0593, Validation Loss: 2.2575\n","Epoch 118, Train Loss: 2.0748, Validation Loss: 2.0882\n","Epoch 119, Train Loss: 2.0595, Validation Loss: 5.2902\n","Epoch 120, Train Loss: 2.0695, Validation Loss: 2.0900\n","Epoch 121, Train Loss: 2.0586, Validation Loss: 2.2591\n","Epoch 122, Train Loss: 2.1602, Validation Loss: 2.1202\n","Epoch 123, Train Loss: 2.0636, Validation Loss: 482.5351\n","Epoch 124, Train Loss: 2.0350, Validation Loss: 1.9215\n","Epoch 125, Train Loss: 2.0019, Validation Loss: 339.8534\n","Epoch 126, Train Loss: 2.0007, Validation Loss: 213.7928\n","Epoch 127, Train Loss: 1.9975, Validation Loss: 46.2493\n","Epoch 128, Train Loss: 1.9416, Validation Loss: 19.0995\n","Epoch 129, Train Loss: 2.1976, Validation Loss: 5.1189\n","Epoch 130, Train Loss: 2.1648, Validation Loss: 15.6599\n","Epoch 131, Train Loss: 2.1638, Validation Loss: 412.2670\n","Epoch 132, Train Loss: 2.0959, Validation Loss: 206.2203\n","Epoch 133, Train Loss: 2.0696, Validation Loss: 68.4666\n","Epoch 134, Train Loss: 2.0825, Validation Loss: 13.2215\n","Epoch 135, Train Loss: 2.0180, Validation Loss: 50.3037\n","Epoch 136, Train Loss: 2.0493, Validation Loss: 31.8954\n","Epoch 137, Train Loss: 2.0717, Validation Loss: 11.7559\n","Epoch 138, Train Loss: 2.0447, Validation Loss: 5.5416\n","Epoch 139, Train Loss: 2.0230, Validation Loss: 46.3891\n","Epoch 140, Train Loss: 2.0206, Validation Loss: 37.4582\n","Epoch 141, Train Loss: 1.9657, Validation Loss: 238.7825\n","Epoch 142, Train Loss: 1.9505, Validation Loss: 3847.1246\n","Epoch 143, Train Loss: 2.0486, Validation Loss: 163.1363\n","Epoch 144, Train Loss: 1.9473, Validation Loss: 338.0715\n","Epoch 145, Train Loss: 2.1998, Validation Loss: 2.1181\n","Epoch 146, Train Loss: 1.9905, Validation Loss: 146.3115\n","Epoch 147, Train Loss: 2.0504, Validation Loss: 2.1714\n","Epoch 148, Train Loss: 1.9020, Validation Loss: 2.0002\n","Epoch 149, Train Loss: 1.8358, Validation Loss: 11.1180\n","Epoch 150, Train Loss: 1.9042, Validation Loss: 2.2019\n","Epoch 151, Train Loss: 2.0544, Validation Loss: 2.5930\n","Epoch 152, Train Loss: 1.9900, Validation Loss: 2.1597\n","Epoch 153, Train Loss: 1.9389, Validation Loss: 7.6609\n","Epoch 154, Train Loss: 1.9405, Validation Loss: 10.5888\n","Epoch 155, Train Loss: 1.8433, Validation Loss: 88.7233\n","Epoch 156, Train Loss: 1.8686, Validation Loss: 5.5258\n","Epoch 157, Train Loss: 1.8438, Validation Loss: 3.6160\n","Epoch 158, Train Loss: 1.9128, Validation Loss: 2.0933\n","Epoch 159, Train Loss: 1.8009, Validation Loss: 2.0107\n","Epoch 160, Train Loss: 1.8149, Validation Loss: 124.3078\n","Epoch 161, Train Loss: 1.7921, Validation Loss: 2.1274\n","Epoch 162, Train Loss: 1.7903, Validation Loss: 2.2157\n","Epoch 163, Train Loss: 1.7714, Validation Loss: 1492.4818\n","Epoch 164, Train Loss: 1.8333, Validation Loss: 3.1607\n","Epoch 165, Train Loss: 1.8263, Validation Loss: 2.2007\n","Epoch 166, Train Loss: 1.9038, Validation Loss: 2.0880\n","Epoch 167, Train Loss: 1.7698, Validation Loss: 1.9856\n","Epoch 168, Train Loss: 1.7561, Validation Loss: 1.9758\n","Epoch 169, Train Loss: 1.7411, Validation Loss: 2.0034\n","Epoch 170, Train Loss: 1.6757, Validation Loss: 2.1387\n","Epoch 171, Train Loss: 1.6512, Validation Loss: 1.8730\n","Epoch 172, Train Loss: 1.6587, Validation Loss: 2.0008\n","Epoch 173, Train Loss: 1.7099, Validation Loss: 2.0115\n","Epoch 174, Train Loss: 1.7004, Validation Loss: 2.5577\n","Epoch 175, Train Loss: 1.7142, Validation Loss: 2.2297\n","Epoch 176, Train Loss: 1.8208, Validation Loss: 2.0553\n","Epoch 177, Train Loss: 1.7425, Validation Loss: 1.9644\n","Epoch 178, Train Loss: 1.7425, Validation Loss: 1.8813\n","Epoch 179, Train Loss: 1.7189, Validation Loss: 2.1182\n","Epoch 180, Train Loss: 1.7236, Validation Loss: 2.1192\n","Epoch 181, Train Loss: 1.7996, Validation Loss: 1842.6318\n","Epoch 182, Train Loss: 1.7284, Validation Loss: 3.1224\n","Epoch 183, Train Loss: 1.6824, Validation Loss: 1.8878\n","Epoch 184, Train Loss: 1.6361, Validation Loss: 5.3067\n","Epoch 185, Train Loss: 1.6146, Validation Loss: 73.2185\n","Epoch 186, Train Loss: 1.5735, Validation Loss: 45.9884\n","Epoch 187, Train Loss: 1.5690, Validation Loss: 7.5075\n","Epoch 188, Train Loss: 1.5885, Validation Loss: 2.8369\n","Epoch 189, Train Loss: 1.5969, Validation Loss: 7.5427\n","Epoch 190, Train Loss: 1.6213, Validation Loss: 1.8929\n","Epoch 191, Train Loss: 1.5893, Validation Loss: 2.3689\n","Epoch 192, Train Loss: 1.5335, Validation Loss: 2.2893\n","Epoch 193, Train Loss: 1.5691, Validation Loss: 3916.0146\n","Epoch 194, Train Loss: 1.8797, Validation Loss: 432.9282\n","Epoch 195, Train Loss: 1.9310, Validation Loss: 4.1086\n","Epoch 196, Train Loss: 1.7122, Validation Loss: 5508.1618\n","Epoch 197, Train Loss: 2.0313, Validation Loss: 7.4808\n","Epoch 198, Train Loss: 1.7437, Validation Loss: 655.1981\n","Epoch 199, Train Loss: 1.6168, Validation Loss: 235.8858\n","Epoch 200, Train Loss: 1.6454, Validation Loss: 12927.7263\n","Epoch 201, Train Loss: 1.6180, Validation Loss: 4013.4755\n","Epoch 202, Train Loss: 1.5993, Validation Loss: 15972.4670\n","Epoch 203, Train Loss: 1.5758, Validation Loss: 3662.8313\n","Epoch 204, Train Loss: 1.5035, Validation Loss: 31112.5423\n","Epoch 205, Train Loss: 1.4804, Validation Loss: 8199.5687\n","Epoch 206, Train Loss: 1.5253, Validation Loss: 7552.1221\n","Epoch 207, Train Loss: 1.5227, Validation Loss: 2910.8341\n","Epoch 208, Train Loss: 1.5505, Validation Loss: 2556.4379\n","Epoch 209, Train Loss: 1.5406, Validation Loss: 2104.0892\n","Epoch 210, Train Loss: 1.5429, Validation Loss: 343.6125\n","Epoch 211, Train Loss: 1.5156, Validation Loss: 16267.7934\n","Epoch 212, Train Loss: 1.5034, Validation Loss: 6989.6674\n","Epoch 213, Train Loss: 1.6855, Validation Loss: 64.6664\n","Epoch 214, Train Loss: 1.6024, Validation Loss: 70.9946\n","Epoch 215, Train Loss: 1.4905, Validation Loss: 2.0231\n","Epoch 216, Train Loss: 1.5185, Validation Loss: 6261.5241\n","Epoch 217, Train Loss: 1.7129, Validation Loss: 4483.6310\n","Epoch 218, Train Loss: 1.5112, Validation Loss: 2.2211\n","Epoch 219, Train Loss: 1.6000, Validation Loss: 17.3945\n","Epoch 220, Train Loss: 1.4690, Validation Loss: 3.9720\n","Epoch 221, Train Loss: 1.5358, Validation Loss: 4909.8039\n","Epoch 222, Train Loss: 1.4555, Validation Loss: 975.1437\n","Epoch 223, Train Loss: 1.5039, Validation Loss: 94.4246\n","Epoch 224, Train Loss: 1.4746, Validation Loss: 1.8545\n","Epoch 225, Train Loss: 1.4603, Validation Loss: 1.8876\n","Epoch 226, Train Loss: 1.4209, Validation Loss: 1.8320\n","Epoch 227, Train Loss: 1.4766, Validation Loss: 1.7777\n","Epoch 228, Train Loss: 1.4128, Validation Loss: 20.8004\n","Epoch 229, Train Loss: 1.4517, Validation Loss: 1.8629\n","Epoch 230, Train Loss: 1.4307, Validation Loss: 2.3020\n","Epoch 231, Train Loss: 1.5409, Validation Loss: 2.2069\n","Epoch 232, Train Loss: 1.4577, Validation Loss: 2.0353\n","Epoch 233, Train Loss: 1.4010, Validation Loss: 2.6165\n","Epoch 234, Train Loss: 1.3971, Validation Loss: 1.8531\n","Epoch 235, Train Loss: 1.3732, Validation Loss: 384.1756\n","Epoch 236, Train Loss: 1.3832, Validation Loss: 1.8225\n","Epoch 237, Train Loss: 1.4878, Validation Loss: 1.9150\n","Epoch 238, Train Loss: 1.5843, Validation Loss: 74217.1139\n","Epoch 239, Train Loss: 1.4074, Validation Loss: 30486.3226\n","Epoch 240, Train Loss: 1.4539, Validation Loss: 2808.5307\n","Epoch 241, Train Loss: 1.3986, Validation Loss: 24559.0997\n","Epoch 242, Train Loss: 1.3977, Validation Loss: 9048.4724\n","Epoch 243, Train Loss: 1.4032, Validation Loss: 35221.4758\n","Epoch 244, Train Loss: 1.4320, Validation Loss: 6844.1793\n","Epoch 245, Train Loss: 1.3840, Validation Loss: 18705.7996\n","Epoch 246, Train Loss: 1.3473, Validation Loss: 15632.7993\n","Epoch 247, Train Loss: 1.3632, Validation Loss: 58931.1862\n","Epoch 248, Train Loss: 1.4012, Validation Loss: 14782.7052\n","Epoch 249, Train Loss: 1.4307, Validation Loss: 231370.9806\n","Epoch 250, Train Loss: 1.8026, Validation Loss: 2.1334\n","Best model saved to 'best_model_ZINC.pth'.\n"]}],"source":["# Define the model\n","\n","model = GraphGPSModel(input_dim=input_dim, hidden_dim=hidden_dim,  output_dim=output_dim, pe_in_dim=pe_in_dim, pe_out_dim=pe_out_dim, num_layers=num_layers)\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Loss function\n","criterion = nn.MSELoss()\n","\n","# Initialize variables to track the best model\n","best_val_loss = float('inf')\n","best_model = None\n","\n","# Training the model for epochs_num:\n","for epoch in range(epochs_num):\n","    # Train the model for one epoch\n","    train_loss = train()\n","\n","    # Evaluate the model on the validation set\n","    val_loss = evaluate(val_loader)\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Check if this is the best validation loss we've seen\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        # Save a copy of the best model\n","        best_model = model.state_dict()  # No need for deepcopy\n","        print(f'New best model saved at epoch {epoch+1} with Validation Loss: {val_loss:.4f}')\n","\n","# After training, you can save the best model to disk\n","torch.save(best_model, 'best_model_ZINC.pth')\n","print(\"Best model saved to 'best_model_ZINC.pth'.\")"]},{"cell_type":"markdown","metadata":{"id":"ZL5y9FCSW6lS"},"source":["#Test Score:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1852,"status":"ok","timestamp":1731836125570,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"},"user_tz":-120},"id":"ZxbpcguRVAIt","outputId":"cc58568f-9201-4c00-be4b-f2382ff2519b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test MAE: 1.0756\n"]}],"source":["def test_score():\n","    model.eval()  # Set the model to evaluation mode\n","    total_mae = 0.0\n","    num_batches = 0\n","\n","    with torch.no_grad():  # Disable gradient computation during evaluation\n","        for batch in test_loader:  # Assuming you have a DataLoader for your test set\n","            # Move batch data to the same device as the model\n","            data = batch.to(device)\n","\n","            # Forward pass (prediction)\n","            output = model(data)\n","\n","            # Ensure target is the correct shape\n","            target = data.y.view(-1, 1).to(device)  # Match output shape: [batch_size, 1]\n","\n","            # Compute Mean Absolute Error (MAE)\n","            mae_loss = F.l1_loss(output, target)\n","\n","            total_mae += mae_loss.item()\n","            num_batches += 1\n","\n","    # Return average MAE over all batches in the test set\n","    avg_mae = total_mae / num_batches\n","    return avg_mae\n","\n","\n","# Load the best model's state dictionary\n","model.load_state_dict(best_model)\n","model.to(device)  # Ensure the model is on the correct device (GPU or CPU)\n","\n","# Now you can evaluate the model on the test set\n","test_mae = test_score()\n","print(f\"Test MAE: {test_mae:.4f}\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1QFTYhfxxfAVTGws9J3OC3vfMfiswZd4S","timestamp":1731750441975},{"file_id":"1xQXVlK-CfZxyLG6gSleHOv0PMZmbdRDB","timestamp":1731683200133}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}