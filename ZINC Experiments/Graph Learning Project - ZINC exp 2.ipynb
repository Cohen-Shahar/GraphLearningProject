{"cells":[{"cell_type":"markdown","metadata":{"id":"XuXWJLEm2UWS"},"source":["# **Graph Learning Project - ZINC exp 2**"]},{"cell_type":"markdown","metadata":{"id":"8gzsP50bF6Gb"},"source":["By Shahar Cohen 205669260 & Alexander petrunin 205782568"]},{"cell_type":"markdown","metadata":{"id":"67gOQITlCNQi"},"source":["# Installation"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3995,"status":"ok","timestamp":1731749526416,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"},"user_tz":-120},"id":"J_m9l6OYCQZP","outputId":"f489a117-8d3e-4ad5-ee37-f3918989a555"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch-geometric"]},{"cell_type":"markdown","source":["# SETUP"],"metadata":{"id":"9V_JqF8wNr1q"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"PRfgbfTjCRD_","executionInfo":{"status":"ok","timestamp":1731749626848,"user_tz":-120,"elapsed":1032,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch_geometric.nn import GPSConv, GatedGraphConv, TransformerConv, GINEConv\n","from torch_geometric.data import Data\n","import torch.nn.functional as F\n","from torch_geometric.nn import global_mean_pool, global_add_pool\n","\n","from torch_geometric.transforms import AddLaplacianEigenvectorPE\n","import torch_geometric\n","\n","import torch.optim as optim\n","\n","from torch_geometric.datasets import ZINC\n","from torch_geometric.loader import DataLoader\n","\n","import torch.optim as optim\n","from torch_geometric.data import DataLoader\n","from sklearn.metrics import mean_squared_error\n","\n","from torch_geometric.transforms import AddRandomWalkPE\n","from torch_geometric.datasets import ZINC\n","from torch_geometric.data import DataLoader\n","\n","from torch_geometric.typing import Tensor\n","from torch_geometric.typing import Adj\n","from typing import Any, Dict, Optional\n","from torch_geometric.utils import to_dense_batch\n","from torch_geometric.nn.attention import PerformerAttention\n"]},{"cell_type":"markdown","source":["# MODEL:"],"metadata":{"id":"nxbRfIBwj8v2"}},{"cell_type":"code","source":["class MLPBlock(nn.Module):\n","    def __init__(self, in_channels, hidden_channels):\n","        super(MLPBlock, self).__init__()\n","        self.fc1 = nn.Linear(in_channels, hidden_channels)\n","        self.fc2 = nn.Linear(hidden_channels, hidden_channels)  # This should output 'hidden_channels'\n","\n","    def forward(self, x):\n","        x = x.float()  # Ensure the input is float before passing it to the linear layer\n","        x = F.relu(self.fc1(x))  # Apply ReLU activation after the first linear layer\n","        x = self.fc2(x)  # The second layer keeps the number of features as hidden_channels\n","        return x\n"],"metadata":{"id":"WLjIZKJZqrk5","executionInfo":{"status":"ok","timestamp":1731749695696,"user_tz":-120,"elapsed":1058,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class GraphGPSModel(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim, pe_in_dim, pe_out_dim, num_layers):\n","        super(GraphGPSModel, self).__init__()\n","\n","        # MLP layers\n","        self.mlp1 = MLPBlock(input_dim + pe_out_dim, hidden_dim)\n","\n","        # Create MLP layers for GINEConv GPSConv layers\n","        self.mlps = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(hidden_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, hidden_dim)\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Create GPSConv layers\n","        self.gps_layers = nn.ModuleList([\n","            CustomGPSConv(\n","                hidden_dim,\n","                conv=GINEConv(self.mlps[i], eps=0.0, train_eps=False, edge_dim=3),\n","                heads=4,\n","                attn_kwargs={'dropout': 0.5}\n","            )\n","            for i in range(num_layers)\n","        ])\n","\n","        # Final fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","        # PE layers\n","        self.bn_pe = nn.BatchNorm1d(pe_in_dim)\n","        self.fc_pe = nn.Linear(pe_in_dim, pe_out_dim)\n","\n","\n","\n","    def forward(self, data):\n","\n","        x, edge_index, batch, pe, edge_attr = (\n","            data.x,\n","            data.edge_index,\n","            data.batch,\n","            data.random_walk_pe,\n","            data.edge_attr\n","        )\n","\n","        # Transform edge attributes\n","        dummy_tensor = torch.zeros(edge_attr.size(0), 3, device=edge_attr.device)  # Change size to 3 for the new dummy\n","        dummy_tensor[edge_attr == 1, 0] = 1  # Keep the condition for edge_attr == 1\n","        dummy_tensor[edge_attr == 2, 1] = 1  # Keep the condition for edge_attr == 2\n","        dummy_tensor[edge_attr == 3, 2] = 1  # New condition for edge_attr == 3\n","        edge_attr = dummy_tensor\n","\n","\n","        # Process positional encodings (PE)\n","        pe = self.bn_pe(pe)\n","        pe = self.fc_pe(pe)\n","\n","        # Concatenate PE to node features\n","        x = torch.cat([x, pe], dim=1)\n","\n","        # Initial MLP processing\n","        x = self.mlp1(x)\n","\n","        # Sequentially apply GPSConv layers\n","        for gps_layer in self.gps_layers:\n","            x = gps_layer(x, edge_index, batch=batch, edge_attr=edge_attr)\n","\n","        # Global pooling to aggregate node features into graph features\n","        x = global_add_pool(x, batch)\n","\n","        # Final classification layer\n","        x = self.fc(x)\n","        return x\n"],"metadata":{"id":"uf4G1gD1SLTZ","executionInfo":{"status":"ok","timestamp":1731749696282,"user_tz":-120,"elapsed":1,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["GPS layer"],"metadata":{"id":"-22Lvmw4STN0"}},{"cell_type":"code","source":["class CustomGPSConv(GPSConv):\n","    def __init__(self,hidden_dim, *args, **kwargs):\n","        super().__init__(hidden_dim, *args, **kwargs)\n","        self.MLP_combine = MLPBlock(hidden_dim*2, hidden_dim)\n","\n","    def forward(\n","        self,\n","        x: Tensor,\n","        edge_index: Adj,\n","        batch: Optional[torch.Tensor] = None,\n","        **kwargs,\n","    ) -> Tensor:\n","        r\"\"\"Runs the forward pass of the module.\"\"\"\n","        hs = []\n","        if self.conv is not None:  # Local MPNN.\n","            h = self.conv(x, edge_index, **kwargs)\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","            h = h + x\n","            if self.norm1 is not None:\n","                if self.norm_with_batch:\n","                    h = self.norm1(h, batch=batch)\n","                else:\n","                    h = self.norm1(h)\n","            hs.append(h)\n","\n","        # Global attention transformer-style model.\n","        h, mask = to_dense_batch(x, batch)\n","\n","        if isinstance(self.attn, torch.nn.MultiheadAttention):\n","            h, _ = self.attn(h, h, h, key_padding_mask=~mask,\n","                             need_weights=False)\n","        elif isinstance(self.attn, PerformerAttention):\n","            h = self.attn(h, mask=mask)\n","\n","        h = h[mask]\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","        h = h + x  # Residual connection.\n","        if self.norm2 is not None:\n","            if self.norm_with_batch:\n","                h = self.norm2(h, batch=batch)\n","            else:\n","                h = self.norm2(h)\n","        hs.append(h)\n","\n","\n","        ##################\n","\n","        # Original Code:\n","        #out = sum(hs)  # Combine local and global outputs.\n","\n","        # Our code:\n","\n","        # Combine the two tensors in hs by concatenating them along the feature dimension (dim=1)\n","        out = torch.cat(hs, dim=1)\n","\n","        # Pass the combined tensor through the MLP\n","        out = self.MLP_combine(out)\n","\n","        # Original Code:\n","        #out = out + self.mlp(out)\n","\n","        #################\n","\n","\n","        if self.norm3 is not None:\n","            if self.norm_with_batch:\n","                out = self.norm3(out, batch=batch)\n","            else:\n","                out = self.norm3(out)\n","\n","        return out"],"metadata":{"id":"7jjLxaasSSHb","executionInfo":{"status":"ok","timestamp":1731749698335,"user_tz":-120,"elapsed":1,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Load ZINC and add PE:"],"metadata":{"id":"tmg5-2VbSSvX"}},{"cell_type":"code","source":["# Load the ZINC dataset\n","transform = AddRandomWalkPE(walk_length=20)\n","\n","# Load the ZINC dataset with predefined splits\n","train_dataset = ZINC(root='./data', subset=True, split='train', transform=transform)\n","val_dataset = ZINC(root='./data', subset=True, split='val', transform=transform)\n","test_dataset = ZINC(root='./data', subset=True, split='test', transform=transform)\n","\n","# Create DataLoaders for batching\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONAcZhFhkXwC","executionInfo":{"status":"ok","timestamp":1731749915942,"user_tz":-120,"elapsed":86253,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"758e40c3-b666-4baf-d50e-68cd73839f95"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.dropbox.com/s/feo9qle74kg48gy/molecules.zip?dl=1\n","Extracting data/molecules.zip\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/train.index\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/val.index\n","Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/test.index\n","Processing...\n","Processing train dataset: 100%|██████████| 10000/10000 [00:00<00:00, 12936.73it/s]\n","Processing val dataset: 100%|██████████| 1000/1000 [00:00<00:00, 4036.97it/s]\n","Processing test dataset: 100%|██████████| 1000/1000 [00:00<00:00, 8634.82it/s]\n","Done!\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"5fhLOTceSr9p"}},{"cell_type":"code","source":["# Training loop\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        optimizer.zero_grad()\n","\n","        # Move data to the same device as the model\n","        data = data.to(device)\n","\n","        # Forward pass\n","        output = model(data)\n","\n","        # Get the target values (penalized logP)\n","        y = data.y.view(-1, 1).to(device)  # Ensure target is on the same device as the model\n","\n","        # Compute the loss\n","        loss = criterion(output, y)\n","        loss.backward()\n","\n","        # Optimization step\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","\n","    return total_loss / len(train_loader)\n","\n","\n","# Define a function to evaluate the model on a given dataset\n","def evaluate(loader):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():  # Disable gradient computation for evaluation\n","        for data in loader:\n","            data = data.to(device)\n","            output = model(data)\n","            y = data.y.view(-1, 1).to(device)\n","            loss = criterion(output, y)\n","            total_loss += loss.item()\n","    return total_loss / len(loader)\n"],"metadata":{"id":"WLHHpsekXog2","executionInfo":{"status":"ok","timestamp":1731750001239,"user_tz":-120,"elapsed":639,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["num_layers = 10\n","input_dim = train_dataset.num_features\n","hidden_dim = 64\n","output_dim = 1\n","pe_in_dim = 20\n","pe_out_dim = 28\n","\n","weight_decay = 1e-5\n","lr = 0.001\n","epochs_num = 250"],"metadata":{"id":"vaMtoPSbVADf","executionInfo":{"status":"ok","timestamp":1731752391870,"user_tz":-120,"elapsed":491,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","\n","model = GraphGPSModel(input_dim=input_dim, hidden_dim=hidden_dim,  output_dim=output_dim, pe_in_dim=pe_in_dim, pe_out_dim=pe_out_dim, num_layers=num_layers)\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Loss function\n","criterion = nn.MSELoss()\n","\n","# Initialize variables to track the best model\n","best_val_loss = float('inf')\n","best_model = None\n","\n","# Training the model for epochs_num:\n","for epoch in range(epochs_num):\n","    # Train the model for one epoch\n","    train_loss = train()\n","\n","    # Evaluate the model on the validation set\n","    val_loss = evaluate(val_loader)\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Check if this is the best validation loss we've seen\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        # Save a copy of the best model\n","        best_model = model.state_dict()  # No need for deepcopy\n","        print(f'New best model saved at epoch {epoch+1} with Validation Loss: {val_loss:.4f}')\n","\n","# After training, you can save the best model to disk\n","torch.save(best_model, 'best_model_ZINC.pth')\n","print(\"Best model saved to 'best_model_ZINC.pth'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__Xx9j2RVAFy","executionInfo":{"status":"ok","timestamp":1731758487089,"user_tz":-120,"elapsed":6094201,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"6a2b1414-7fe7-4f19-ccf1-e34d83731ca2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 2.8845, Validation Loss: 1.5690\n","New best model saved at epoch 1 with Validation Loss: 1.5690\n","Epoch 2, Train Loss: 1.4293, Validation Loss: 1.1748\n","New best model saved at epoch 2 with Validation Loss: 1.1748\n","Epoch 3, Train Loss: 1.1970, Validation Loss: 1.0924\n","New best model saved at epoch 3 with Validation Loss: 1.0924\n","Epoch 4, Train Loss: 1.0791, Validation Loss: 1.1107\n","Epoch 5, Train Loss: 1.0290, Validation Loss: 0.9121\n","New best model saved at epoch 5 with Validation Loss: 0.9121\n","Epoch 6, Train Loss: 0.9854, Validation Loss: 1.5270\n","Epoch 7, Train Loss: 0.9930, Validation Loss: 0.9323\n","Epoch 8, Train Loss: 1.0991, Validation Loss: 1.7170\n","Epoch 9, Train Loss: 1.1739, Validation Loss: 0.9660\n","Epoch 10, Train Loss: 0.9998, Validation Loss: 1.0249\n","Epoch 11, Train Loss: 0.9027, Validation Loss: 0.9292\n","Epoch 12, Train Loss: 0.8869, Validation Loss: 1.4536\n","Epoch 13, Train Loss: 0.9201, Validation Loss: 0.9769\n","Epoch 14, Train Loss: 0.9736, Validation Loss: 0.9982\n","Epoch 15, Train Loss: 0.9463, Validation Loss: 1.1080\n","Epoch 16, Train Loss: 0.9056, Validation Loss: 0.8153\n","New best model saved at epoch 16 with Validation Loss: 0.8153\n","Epoch 17, Train Loss: 0.9555, Validation Loss: 0.8759\n","Epoch 18, Train Loss: 0.9325, Validation Loss: 1.0089\n","Epoch 19, Train Loss: 0.9772, Validation Loss: 1.0373\n","Epoch 20, Train Loss: 0.9259, Validation Loss: 0.8643\n","Epoch 21, Train Loss: 1.1108, Validation Loss: 1.0777\n","Epoch 22, Train Loss: 0.9395, Validation Loss: 1.0191\n","Epoch 23, Train Loss: 0.9342, Validation Loss: 0.7761\n","New best model saved at epoch 23 with Validation Loss: 0.7761\n","Epoch 24, Train Loss: 0.8065, Validation Loss: 0.7958\n","Epoch 25, Train Loss: 0.8920, Validation Loss: 0.8597\n","Epoch 26, Train Loss: 0.8761, Validation Loss: 1.3157\n","Epoch 27, Train Loss: 0.8209, Validation Loss: 1.0324\n","Epoch 28, Train Loss: 0.8730, Validation Loss: 1.3440\n","Epoch 29, Train Loss: 0.8896, Validation Loss: 0.7868\n","Epoch 30, Train Loss: 0.8133, Validation Loss: 0.8667\n","Epoch 31, Train Loss: 0.9902, Validation Loss: 0.8246\n","Epoch 32, Train Loss: 0.8665, Validation Loss: 0.9262\n","Epoch 33, Train Loss: 0.8426, Validation Loss: 0.8331\n","Epoch 34, Train Loss: 0.8808, Validation Loss: 1.1737\n","Epoch 35, Train Loss: 1.1038, Validation Loss: 0.9853\n","Epoch 36, Train Loss: 0.8802, Validation Loss: 0.8535\n","Epoch 37, Train Loss: 0.8408, Validation Loss: 0.9689\n","Epoch 38, Train Loss: 1.0087, Validation Loss: 0.9740\n","Epoch 39, Train Loss: 0.8668, Validation Loss: 0.8121\n","Epoch 40, Train Loss: 0.7938, Validation Loss: 0.8567\n","Epoch 41, Train Loss: 0.7829, Validation Loss: 0.7960\n","Epoch 42, Train Loss: 0.7990, Validation Loss: 0.7053\n","New best model saved at epoch 42 with Validation Loss: 0.7053\n","Epoch 43, Train Loss: 0.7984, Validation Loss: 0.7242\n","Epoch 44, Train Loss: 0.8933, Validation Loss: 1.8092\n","Epoch 45, Train Loss: 1.4393, Validation Loss: 1.0953\n","Epoch 46, Train Loss: 1.0072, Validation Loss: 0.8875\n","Epoch 47, Train Loss: 0.9051, Validation Loss: 0.9195\n","Epoch 48, Train Loss: 0.8436, Validation Loss: 0.7822\n","Epoch 49, Train Loss: 0.7875, Validation Loss: 0.6793\n","New best model saved at epoch 49 with Validation Loss: 0.6793\n","Epoch 50, Train Loss: 0.7626, Validation Loss: 0.7844\n","Epoch 51, Train Loss: 0.7765, Validation Loss: 0.9948\n","Epoch 52, Train Loss: 0.9108, Validation Loss: 1.3233\n","Epoch 53, Train Loss: 0.9402, Validation Loss: 1.1876\n","Epoch 54, Train Loss: 1.0161, Validation Loss: 1.0103\n","Epoch 55, Train Loss: 0.9543, Validation Loss: 0.8857\n","Epoch 56, Train Loss: 0.8055, Validation Loss: 0.7453\n","Epoch 57, Train Loss: 0.7567, Validation Loss: 0.9074\n","Epoch 58, Train Loss: 0.8261, Validation Loss: 0.7210\n","Epoch 59, Train Loss: 0.8346, Validation Loss: 0.9249\n","Epoch 60, Train Loss: 0.7907, Validation Loss: 0.8095\n","Epoch 61, Train Loss: 0.7621, Validation Loss: 0.8839\n","Epoch 62, Train Loss: 0.8357, Validation Loss: 1.1117\n","Epoch 63, Train Loss: 0.8078, Validation Loss: 1.0299\n","Epoch 64, Train Loss: 0.7109, Validation Loss: 0.7155\n","Epoch 65, Train Loss: 0.6876, Validation Loss: 0.8401\n","Epoch 66, Train Loss: 0.6881, Validation Loss: 18.2568\n","Epoch 67, Train Loss: 1.6085, Validation Loss: 2.5298\n","Epoch 68, Train Loss: 2.4356, Validation Loss: 2.5762\n","Epoch 69, Train Loss: 2.4078, Validation Loss: 2.3824\n","Epoch 70, Train Loss: 2.3607, Validation Loss: 2.4011\n","Epoch 71, Train Loss: 2.3086, Validation Loss: 2.2296\n","Epoch 72, Train Loss: 2.2799, Validation Loss: 2.2983\n","Epoch 73, Train Loss: 2.2610, Validation Loss: 2.2000\n","Epoch 74, Train Loss: 2.2531, Validation Loss: 2.1512\n","Epoch 75, Train Loss: 2.2228, Validation Loss: 2.1746\n","Epoch 76, Train Loss: 2.1841, Validation Loss: 2.1946\n","Epoch 77, Train Loss: 2.1779, Validation Loss: 2.1525\n","Epoch 78, Train Loss: 2.1571, Validation Loss: 2.1235\n","Epoch 79, Train Loss: 2.1746, Validation Loss: 2.0592\n","Epoch 80, Train Loss: 2.1267, Validation Loss: 2.4721\n","Epoch 81, Train Loss: 2.1316, Validation Loss: 2.0179\n","Epoch 82, Train Loss: 2.0897, Validation Loss: 2.1518\n","Epoch 83, Train Loss: 2.0863, Validation Loss: 2.1751\n","Epoch 84, Train Loss: 2.0740, Validation Loss: 2.4032\n","Epoch 85, Train Loss: 2.0960, Validation Loss: 1.9614\n","Epoch 86, Train Loss: 2.0257, Validation Loss: 1.9805\n","Epoch 87, Train Loss: 2.0421, Validation Loss: 2.0211\n","Epoch 88, Train Loss: 2.0717, Validation Loss: 1.9550\n","Epoch 89, Train Loss: 1.9599, Validation Loss: 1.9617\n","Epoch 90, Train Loss: 1.8695, Validation Loss: 1.8571\n","Epoch 91, Train Loss: 1.5415, Validation Loss: 1.7452\n","Epoch 92, Train Loss: 1.4366, Validation Loss: 1.4740\n","Epoch 93, Train Loss: 1.3287, Validation Loss: 1.2171\n","Epoch 94, Train Loss: 1.2208, Validation Loss: 1.4495\n","Epoch 95, Train Loss: 1.1940, Validation Loss: 1.3900\n","Epoch 96, Train Loss: 1.1641, Validation Loss: 1.0977\n","Epoch 97, Train Loss: 1.1257, Validation Loss: 1.1509\n","Epoch 98, Train Loss: 1.0485, Validation Loss: 1.0758\n","Epoch 99, Train Loss: 1.0326, Validation Loss: 1.9660\n","Epoch 100, Train Loss: 1.0164, Validation Loss: 0.9731\n","Epoch 101, Train Loss: 0.9749, Validation Loss: 1.2974\n","Epoch 102, Train Loss: 1.0121, Validation Loss: 0.9362\n","Epoch 103, Train Loss: 0.9045, Validation Loss: 1.0154\n","Epoch 104, Train Loss: 0.8938, Validation Loss: 0.8604\n","Epoch 105, Train Loss: 0.8791, Validation Loss: 1.2641\n","Epoch 106, Train Loss: 0.8958, Validation Loss: 0.8764\n","Epoch 107, Train Loss: 0.8634, Validation Loss: 0.8133\n","Epoch 108, Train Loss: 0.8505, Validation Loss: 1.0165\n","Epoch 109, Train Loss: 0.8438, Validation Loss: 0.8416\n","Epoch 110, Train Loss: 0.8269, Validation Loss: 0.7886\n","Epoch 111, Train Loss: 0.7995, Validation Loss: 0.8815\n","Epoch 112, Train Loss: 0.8061, Validation Loss: 0.7990\n","Epoch 113, Train Loss: 0.7990, Validation Loss: 0.8509\n","Epoch 114, Train Loss: 0.7548, Validation Loss: 0.7805\n","Epoch 115, Train Loss: 0.8434, Validation Loss: 1.3019\n","Epoch 116, Train Loss: 0.8295, Validation Loss: 2.7178\n","Epoch 117, Train Loss: 0.8857, Validation Loss: 0.7689\n","Epoch 118, Train Loss: 0.8158, Validation Loss: 0.7079\n","Epoch 119, Train Loss: 0.7227, Validation Loss: 0.6935\n","Epoch 120, Train Loss: 0.7239, Validation Loss: 0.7977\n","Epoch 121, Train Loss: 0.6583, Validation Loss: 0.7966\n","Epoch 122, Train Loss: 0.7467, Validation Loss: 0.7832\n","Epoch 123, Train Loss: 0.6270, Validation Loss: 0.7587\n","Epoch 124, Train Loss: 0.5510, Validation Loss: 0.8187\n","Epoch 125, Train Loss: 0.5107, Validation Loss: 0.7715\n","Epoch 126, Train Loss: 0.6676, Validation Loss: 0.7622\n","Epoch 127, Train Loss: 0.5722, Validation Loss: 1.1575\n","Epoch 128, Train Loss: 0.5812, Validation Loss: 0.8581\n","Epoch 129, Train Loss: 0.5226, Validation Loss: 0.6561\n","New best model saved at epoch 129 with Validation Loss: 0.6561\n","Epoch 130, Train Loss: 0.5189, Validation Loss: 0.6301\n","New best model saved at epoch 130 with Validation Loss: 0.6301\n","Epoch 131, Train Loss: 0.6533, Validation Loss: 1.0286\n","Epoch 132, Train Loss: 0.5062, Validation Loss: 0.8275\n","Epoch 133, Train Loss: 0.4552, Validation Loss: 0.7810\n","Epoch 134, Train Loss: 0.4756, Validation Loss: 1.0052\n","Epoch 135, Train Loss: 0.5343, Validation Loss: 2.1075\n","Epoch 136, Train Loss: 0.7088, Validation Loss: 1.3123\n","Epoch 137, Train Loss: 0.5897, Validation Loss: 1.2093\n","Epoch 138, Train Loss: 0.5192, Validation Loss: 0.7351\n","Epoch 139, Train Loss: 0.4413, Validation Loss: 0.8093\n","Epoch 140, Train Loss: 0.5547, Validation Loss: 1.3963\n","Epoch 141, Train Loss: 0.5373, Validation Loss: 0.8668\n","Epoch 142, Train Loss: 0.4383, Validation Loss: 1.1277\n","Epoch 143, Train Loss: 0.4251, Validation Loss: 1.0655\n","Epoch 144, Train Loss: 0.6261, Validation Loss: 0.7150\n","Epoch 145, Train Loss: 0.4816, Validation Loss: 0.8897\n","Epoch 146, Train Loss: 0.7031, Validation Loss: 0.8012\n","Epoch 147, Train Loss: 0.6554, Validation Loss: 0.6932\n","Epoch 148, Train Loss: 0.5767, Validation Loss: 0.6719\n","Epoch 149, Train Loss: 0.5157, Validation Loss: 1.6180\n","Epoch 150, Train Loss: 0.7426, Validation Loss: 0.8655\n","Epoch 151, Train Loss: 0.7584, Validation Loss: 0.7891\n","Epoch 152, Train Loss: 0.7902, Validation Loss: 0.7458\n","Epoch 153, Train Loss: 0.5258, Validation Loss: 1.2168\n","Epoch 154, Train Loss: 0.7451, Validation Loss: 0.7881\n","Epoch 155, Train Loss: 0.5242, Validation Loss: 0.8327\n","Epoch 156, Train Loss: 0.5154, Validation Loss: 0.7363\n","Epoch 157, Train Loss: 0.4538, Validation Loss: 0.8290\n","Epoch 158, Train Loss: 0.4389, Validation Loss: 0.7009\n","Epoch 159, Train Loss: 0.4318, Validation Loss: 0.6801\n","Epoch 160, Train Loss: 0.3617, Validation Loss: 0.6329\n","Epoch 161, Train Loss: 0.4527, Validation Loss: 0.6826\n","Epoch 162, Train Loss: 0.4637, Validation Loss: 0.9569\n","Epoch 163, Train Loss: 0.5669, Validation Loss: 0.6825\n","Epoch 164, Train Loss: 0.5141, Validation Loss: 0.6969\n","Epoch 165, Train Loss: 0.3485, Validation Loss: 0.6785\n","Epoch 166, Train Loss: 0.3918, Validation Loss: 0.9629\n","Epoch 167, Train Loss: 0.3330, Validation Loss: 0.7937\n","Epoch 168, Train Loss: 0.4920, Validation Loss: 0.7557\n","Epoch 169, Train Loss: 0.3763, Validation Loss: 0.8237\n","Epoch 170, Train Loss: 0.3842, Validation Loss: 0.7520\n","Epoch 171, Train Loss: 0.3481, Validation Loss: 0.6633\n","Epoch 172, Train Loss: 0.4627, Validation Loss: 0.7287\n","Epoch 173, Train Loss: 0.3929, Validation Loss: 0.9865\n","Epoch 174, Train Loss: 0.3890, Validation Loss: 0.9717\n","Epoch 175, Train Loss: 0.6212, Validation Loss: 0.7321\n","Epoch 176, Train Loss: 0.5935, Validation Loss: 0.9850\n","Epoch 177, Train Loss: 0.8969, Validation Loss: 0.8151\n","Epoch 178, Train Loss: 0.7056, Validation Loss: 0.9802\n","Epoch 179, Train Loss: 0.5136, Validation Loss: 1.0998\n","Epoch 180, Train Loss: 0.4741, Validation Loss: 0.7389\n","Epoch 181, Train Loss: 0.4995, Validation Loss: 0.8153\n","Epoch 182, Train Loss: 0.4949, Validation Loss: 0.8438\n","Epoch 183, Train Loss: 0.4256, Validation Loss: 1.3966\n","Epoch 184, Train Loss: 0.3661, Validation Loss: 0.6866\n","Epoch 185, Train Loss: 0.3637, Validation Loss: 0.7434\n","Epoch 186, Train Loss: 0.3656, Validation Loss: 0.6781\n","Epoch 187, Train Loss: 0.3630, Validation Loss: 0.7625\n","Epoch 188, Train Loss: 0.4226, Validation Loss: 0.9391\n","Epoch 189, Train Loss: 0.3653, Validation Loss: 0.7382\n","Epoch 190, Train Loss: 0.4061, Validation Loss: 0.7971\n","Epoch 191, Train Loss: 0.3688, Validation Loss: 0.6500\n","Epoch 192, Train Loss: 0.3992, Validation Loss: 0.8961\n","Epoch 193, Train Loss: 0.4202, Validation Loss: 0.9235\n","Epoch 194, Train Loss: 0.5105, Validation Loss: 0.7773\n","Epoch 195, Train Loss: 0.6780, Validation Loss: 0.8173\n","Epoch 196, Train Loss: 0.4838, Validation Loss: 1.0211\n","Epoch 197, Train Loss: 0.3697, Validation Loss: 0.8489\n","Epoch 198, Train Loss: 0.3744, Validation Loss: 0.9228\n","Epoch 199, Train Loss: 0.3759, Validation Loss: 0.9588\n","Epoch 200, Train Loss: 0.3718, Validation Loss: 0.6747\n","Epoch 201, Train Loss: 0.3494, Validation Loss: 1.0361\n","Epoch 202, Train Loss: 0.5354, Validation Loss: 3.4591\n","Epoch 203, Train Loss: 0.5884, Validation Loss: 0.6672\n","Epoch 204, Train Loss: 0.4828, Validation Loss: 0.7117\n","Epoch 205, Train Loss: 0.4134, Validation Loss: 0.6525\n","Epoch 206, Train Loss: 0.3532, Validation Loss: 0.9192\n","Epoch 207, Train Loss: 0.3341, Validation Loss: 0.7497\n","Epoch 208, Train Loss: 0.3731, Validation Loss: 0.7330\n","Epoch 209, Train Loss: 0.3248, Validation Loss: 1.0473\n","Epoch 210, Train Loss: 0.3353, Validation Loss: 0.7457\n","Epoch 211, Train Loss: 0.2807, Validation Loss: 0.7806\n","Epoch 212, Train Loss: 0.3170, Validation Loss: 0.8640\n","Epoch 213, Train Loss: 0.4517, Validation Loss: 0.6828\n","Epoch 214, Train Loss: 0.5281, Validation Loss: 0.6227\n","New best model saved at epoch 214 with Validation Loss: 0.6227\n","Epoch 215, Train Loss: 0.3781, Validation Loss: 0.7472\n","Epoch 216, Train Loss: 0.4579, Validation Loss: 0.7408\n","Epoch 217, Train Loss: 0.3391, Validation Loss: 0.6477\n","Epoch 218, Train Loss: 0.3687, Validation Loss: 0.7158\n","Epoch 219, Train Loss: 0.3151, Validation Loss: 0.8109\n","Epoch 220, Train Loss: 0.3049, Validation Loss: 1.1412\n","Epoch 221, Train Loss: 0.3369, Validation Loss: 1.2183\n","Epoch 222, Train Loss: 0.4030, Validation Loss: 0.9641\n","Epoch 223, Train Loss: 0.5465, Validation Loss: 0.8292\n","Epoch 224, Train Loss: 0.3958, Validation Loss: 0.8277\n","Epoch 225, Train Loss: 0.3116, Validation Loss: 0.7855\n","Epoch 226, Train Loss: 0.3083, Validation Loss: 0.7177\n","Epoch 227, Train Loss: 0.5076, Validation Loss: 4.9674\n","Epoch 228, Train Loss: 0.4213, Validation Loss: 0.6936\n","Epoch 229, Train Loss: 0.4031, Validation Loss: 0.7631\n","Epoch 230, Train Loss: 0.3959, Validation Loss: 1.0084\n","Epoch 231, Train Loss: 0.2923, Validation Loss: 0.7442\n","Epoch 232, Train Loss: 0.2983, Validation Loss: 0.6160\n","New best model saved at epoch 232 with Validation Loss: 0.6160\n","Epoch 233, Train Loss: 0.2908, Validation Loss: 0.6550\n","Epoch 234, Train Loss: 0.2695, Validation Loss: 0.6611\n","Epoch 235, Train Loss: 0.2619, Validation Loss: 0.6654\n","Epoch 236, Train Loss: 0.3217, Validation Loss: 0.6460\n","Epoch 237, Train Loss: 0.3344, Validation Loss: 1.2150\n","Epoch 238, Train Loss: 0.5016, Validation Loss: 0.6510\n","Epoch 239, Train Loss: 0.5074, Validation Loss: 0.6840\n","Epoch 240, Train Loss: 0.4728, Validation Loss: 0.6804\n","Epoch 241, Train Loss: 0.4265, Validation Loss: 0.5787\n","New best model saved at epoch 241 with Validation Loss: 0.5787\n","Epoch 242, Train Loss: 0.2837, Validation Loss: 0.7720\n","Epoch 243, Train Loss: 0.2419, Validation Loss: 0.7411\n","Epoch 244, Train Loss: 0.2711, Validation Loss: 0.5779\n","New best model saved at epoch 244 with Validation Loss: 0.5779\n","Epoch 245, Train Loss: 0.4806, Validation Loss: 1.4375\n","Epoch 246, Train Loss: 0.3430, Validation Loss: 0.5909\n","Epoch 247, Train Loss: 0.4545, Validation Loss: 0.6980\n","Epoch 248, Train Loss: 0.7399, Validation Loss: 0.7727\n","Epoch 249, Train Loss: 0.7024, Validation Loss: 1.3051\n","Epoch 250, Train Loss: 0.4647, Validation Loss: 0.9358\n","Best model saved to 'best_model_ZINC.pth'.\n"]}]},{"cell_type":"markdown","source":["#Test Score:"],"metadata":{"id":"ZL5y9FCSW6lS"}},{"cell_type":"code","source":["def test_score():\n","    model.eval()  # Set the model to evaluation mode\n","    total_mae = 0.0\n","    num_batches = 0\n","\n","    with torch.no_grad():  # Disable gradient computation during evaluation\n","        for batch in test_loader:  # Assuming you have a DataLoader for your test set\n","            # Move batch data to the same device as the model\n","            data = batch.to(device)\n","\n","            # Forward pass (prediction)\n","            output = model(data)\n","\n","            # Ensure target is the correct shape\n","            target = data.y.view(-1, 1).to(device)  # Match output shape: [batch_size, 1]\n","\n","            # Compute Mean Absolute Error (MAE)\n","            mae_loss = F.l1_loss(output, target)\n","\n","            total_mae += mae_loss.item()\n","            num_batches += 1\n","\n","    # Return average MAE over all batches in the test set\n","    avg_mae = total_mae / num_batches\n","    return avg_mae\n","\n","\n","# Load the best model's state dictionary\n","model.load_state_dict(best_model)\n","model.to(device)  # Ensure the model is on the correct device (GPU or CPU)\n","\n","# Now you can evaluate the model on the test set\n","test_mae = test_score()\n","print(f\"Test MAE: {test_mae:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxbpcguRVAIt","executionInfo":{"status":"ok","timestamp":1731758520516,"user_tz":-120,"elapsed":479,"user":{"displayName":"שחר כהן","userId":"02544236883128860612"}},"outputId":"724b9d2b-465f-4194-a98d-6818284e66da"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Test MAE: 0.3032\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1xQXVlK-CfZxyLG6gSleHOv0PMZmbdRDB","timestamp":1731683200133}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}